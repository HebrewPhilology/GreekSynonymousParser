{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d942dc31-03a7-4d27-a5ad-0a312beaaa26",
   "metadata": {},
   "outputs": [],
   "source": [
    "Requirements\n",
    "\n",
    "requests\n",
    "bs4\n",
    "lxml\n",
    "numpy\n",
    "pandas\n",
    "\n",
    "Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63625f1f-e6c8-480c-ab3a-91ded859c0a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "from string import digits\n",
    "from bs4 import BeautifulSoup\n",
    "from numpy import loadtxt\n",
    "\n",
    "\n",
    "#Config\n",
    "\n",
    "word_list = []\n",
    "word_list = loadtxt('en_words.txt', dtype='str')\n",
    "occur_min = 2000\n",
    "occur_max = 100000\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def import_perseus(en_lex):\n",
    "    \n",
    "    #1. Calcule le nombre de pages html\n",
    "    url = \"https://www.perseus.tufts.edu/hopper/definitionlookup?&q=\"+en_lex+\"&sort=freq&target=greek\"\n",
    "\n",
    "    data = requests.get(url)\n",
    "    if data is None:\n",
    "        return\n",
    "    soup = BeautifulSoup(data.text, 'lxml')                          #Extract in soup the html of the page\n",
    "    try:\n",
    "        pages = max(list(soup.find(\"div\", {\"class\":\"pager\"}).text))      #Extract the number of result pages\n",
    "    except:\n",
    "        pages = 1\n",
    "    #2. Récupère le DataFrame pour chaque page html\n",
    "\n",
    "    df=[]\n",
    "    for x in range(int(pages)):\n",
    "        url = \"https://www.perseus.tufts.edu/hopper/definitionlookup?type=exact&page=\"+str(x+1)+\"&q=\"+en_lex+\"&sort=freq&target=greek\"\n",
    "        data = requests.get(url)\n",
    "        soup = BeautifulSoup(data.text, 'lxml')                       #Extract in soup the html of the page\n",
    "        table = soup.find(\"table\", {\"class\":\"data\"})                  #Extract in table the html of the table \"data\"\n",
    "        if table is None:\n",
    "            return\n",
    "        \n",
    "        rows = []\n",
    "        for child in table.children:\n",
    "            row = []\n",
    "            for td in child:\n",
    "                try:\n",
    "                    row.append(td.text.replace('\\n', ''))\n",
    "                    \n",
    "                except:\n",
    "                    continue\n",
    "            if len(row) > 0:\n",
    "                rows.append(row)\n",
    "                \n",
    "        df_part = pd.DataFrame(rows[1:], columns=rows[0])             #Create the Dataframe for each page\n",
    "        df.append(df_part)                                            #Add the Dataframe into a list\n",
    "   \n",
    "    result=pd.concat(df, axis=0, ignore_index=True)                   #Concatenate the list into one Dataframe\n",
    "\n",
    "    \n",
    "    #3. Filtrage des données\n",
    "    result['Max. Inst.'] = result['Max. Inst.'].str.replace(',','')   #Supprime les virgules dans la colonne Max. Inst.\n",
    "    #result['Headword'] = re.sub(r'[0-9]+', '', result['Headword'].str)\n",
    "    \n",
    "    result = result.astype({'Max. Inst.':'int'})                      #Convertit les valeurs de la colonne Max. Inst. en integer\n",
    "    \n",
    "    result = result[(result['Short Definition'] == en_lex+\",\")\n",
    "                    | (result['Short Definition'] == en_lex) \n",
    "                    | (result['Short Definition'] == \", \"+en_lex) \n",
    "                    | ((result['Max. Inst.'] > occur_min) & (result['Max. Inst.'] < occur_max))]\n",
    "    if result.empty:\n",
    "        return\n",
    "    \n",
    "    result = result.sort_values(by = 'Max. Inst.', ascending = False) #Trie les valeurs par Max. Inst. décroissant\n",
    "\n",
    "    #print(result.loc[:, [\"Headword\", \"Max. Inst.\", \"Short Definition\"]])\n",
    "    \n",
    "    \n",
    "    \n",
    "    #4. Dump les valeurs dans le dictionnaire\n",
    "        \n",
    "    result = result.rename(columns = {'Headword': en_lex})          #Renome la colonne des mots grec par sa clé en anglais\n",
    "\n",
    "    #dict_syn[en_lex] = en_lex\n",
    "    dict_syn = result.loc[:, [en_lex]].to_dict(orient='list')     #Dump la colonne des synonymes dans le dictionnaire\n",
    "    \n",
    "    dict_syn = {key: [re.sub('\\d', '', ele) for ele in val]         #Nettoie le dictionnaire de toute valeur numérique\n",
    "       for key, val in dict_syn.items()}\n",
    "    \n",
    "    dict_syn[dict_syn[en_lex][0]]=dict_syn[en_lex]\n",
    "    del dict_syn[en_lex][0]\n",
    "    del dict_syn[en_lex]\n",
    "    \n",
    "    return dict_syn\n",
    "\n",
    "\n",
    "\n",
    "dict_syns = {}\n",
    "i=0\n",
    "count=0\n",
    "for en_lex in word_list:\n",
    "\n",
    "    if en_lex!=\"\":\n",
    "        dict_syn = import_perseus(en_lex)\n",
    "    if dict_syn: dict_syns.update(dict_syn)\n",
    "    \n",
    "    #Créer un dump json tout les 10 mots\n",
    "    if i==10:\n",
    "        with open(\"synonyms.txt\", \"w\", encoding='utf-8') as f:\n",
    "            json.dump(dict_syns, f, ensure_ascii=False)\n",
    "            print(\"dump\", end=' ')\n",
    "            \n",
    "            \n",
    "        i=0\n",
    "    i+=1\n",
    "    count+=1\n",
    "    print(count, end=' ')\n",
    "\n",
    "#dump final    \n",
    "with open(\"synonyms.txt\", \"a+\", encoding='utf-8') as f:\n",
    "    json.dump(dict_syns, f, ensure_ascii=False)    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "a2aa0cd5-b54e-442e-97bb-bda81cf851a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for 5 words: 13.919054746627808 s\n",
      "Time for 5 words: 14.094571113586426 s\n",
      "Time for 5 words: 14.244662046432495 s\n",
      "Time for 5 words: 14.311150074005127 s\n",
      "Time for 5 words: 14.090303897857666 s\n"
     ]
    }
   ],
   "source": [
    "for bench in range(5):\n",
    "    start = time.time()\n",
    "    for en_lex in word_list:\n",
    "        test = ImportDataPerseus(en_lex)\n",
    "    stop = time.time()\n",
    "        \n",
    "    print(f\"Time for {len(word_list)} words: {stop - start} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de69e2ef-8c4d-4b85-8d7b-e705420e0051",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"en_words.txt\", \"r\")\n",
    "word_list = f.read()\n",
    "print(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ff6591-15be-4774-810b-4e908a7a2403",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Parser",
   "language": "python",
   "name": "parser"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
